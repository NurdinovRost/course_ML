{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda = models.LdaModel(corpus, num_topics=40, passes=5, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_lda_topics_0 = lda.show_topics(num_topics=40, num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('chopped onion', 0.099176206),\n",
       "   ('garlic cloves', 0.07797448),\n",
       "   ('salt', 0.05860214),\n",
       "   ('fat free less sodium chicken broth', 0.051822294),\n",
       "   ('sliced green onions', 0.046389583),\n",
       "   ('water', 0.043945324),\n",
       "   ('cooking spray', 0.04244567),\n",
       "   ('spinach', 0.03678857),\n",
       "   ('ground red pepper', 0.03600164),\n",
       "   ('green beans', 0.03593165)]),\n",
       " (1,\n",
       "  [('corn kernels', 0.10112804),\n",
       "   ('diced onions', 0.08601628),\n",
       "   ('tortillas', 0.072016545),\n",
       "   ('vegetable stock', 0.06619621),\n",
       "   ('chopped fresh chives', 0.049702685),\n",
       "   ('sliced black olives', 0.049264804),\n",
       "   ('cream cheese, soften', 0.04781),\n",
       "   ('lard', 0.04337834),\n",
       "   ('jack cheese', 0.034408577),\n",
       "   ('canned black beans', 0.024252873)]),\n",
       " (2,\n",
       "  [('olive oil', 0.09124677),\n",
       "   ('crushed red pepper', 0.06121788),\n",
       "   ('fresh parsley', 0.0603052),\n",
       "   ('garlic cloves', 0.04248359),\n",
       "   ('grated parmesan cheese', 0.04088032),\n",
       "   ('butter', 0.039902076),\n",
       "   ('cherry tomatoes', 0.036582854),\n",
       "   ('salt', 0.035435695),\n",
       "   ('low salt chicken broth', 0.03513021),\n",
       "   ('fresh rosemary', 0.03347208)]),\n",
       " (3,\n",
       "  [('bacon', 0.07488386),\n",
       "   ('salt', 0.073459215),\n",
       "   ('red pepper flakes', 0.061836123),\n",
       "   ('onions', 0.054242127),\n",
       "   ('garlic', 0.05066283),\n",
       "   ('ground black pepper', 0.048926868),\n",
       "   ('chicken thighs', 0.04325552),\n",
       "   ('pasta', 0.042299848),\n",
       "   ('olive oil', 0.0338725),\n",
       "   ('mushrooms', 0.02848959)]),\n",
       " (4,\n",
       "  [('cooking spray', 0.09330629),\n",
       "   ('salt', 0.06754125),\n",
       "   ('powdered sugar', 0.061182782),\n",
       "   ('all-purpose flour', 0.05381205),\n",
       "   ('large egg whites', 0.050456893),\n",
       "   ('sugar', 0.04965256),\n",
       "   ('large eggs', 0.049361862),\n",
       "   ('cream cheese', 0.044328347),\n",
       "   ('yellow corn meal', 0.04394412),\n",
       "   ('butter', 0.041056756)]),\n",
       " (5,\n",
       "  [('shallots', 0.09258227),\n",
       "   ('large garlic cloves', 0.09237794),\n",
       "   ('dry white wine', 0.07489166),\n",
       "   ('olive oil', 0.056517042),\n",
       "   ('finely chopped onion', 0.04483457),\n",
       "   ('salt', 0.033820827),\n",
       "   ('unsalted butter', 0.032587364),\n",
       "   ('white wine vinegar', 0.031888373),\n",
       "   ('arborio rice', 0.03049748),\n",
       "   ('saffron threads', 0.026581619)]),\n",
       " (6,\n",
       "  [('fresh thyme', 0.07946837),\n",
       "   ('dry red wine', 0.07852518),\n",
       "   ('pork tenderloin', 0.053433444),\n",
       "   ('reduced sodium soy sauce', 0.047641575),\n",
       "   ('cilantro sprigs', 0.047100365),\n",
       "   ('beef broth', 0.04125771),\n",
       "   ('peppercorns', 0.036827616),\n",
       "   ('cremini mushrooms', 0.032652363),\n",
       "   ('rosemary', 0.023344237),\n",
       "   ('daikon', 0.023007965)]),\n",
       " (7,\n",
       "  [('soy sauce', 0.092063904),\n",
       "   ('sesame oil', 0.05160112),\n",
       "   ('scallions', 0.043488186),\n",
       "   ('green onions', 0.042372525),\n",
       "   ('rice vinegar', 0.041117795),\n",
       "   ('sugar', 0.039605554),\n",
       "   ('corn starch', 0.036382463),\n",
       "   ('garlic', 0.035425365),\n",
       "   ('vegetable oil', 0.03349449),\n",
       "   ('fresh ginger', 0.027111534)]),\n",
       " (8,\n",
       "  [('garlic powder', 0.17439853),\n",
       "   ('cayenne pepper', 0.121108234),\n",
       "   ('onion powder', 0.06542909),\n",
       "   ('ground black pepper', 0.04715858),\n",
       "   ('smoked paprika', 0.043415975),\n",
       "   ('black pepper', 0.040444683),\n",
       "   ('pinenuts', 0.03994136),\n",
       "   ('salt', 0.03808908),\n",
       "   ('fresh spinach', 0.029073106),\n",
       "   ('dried oregano', 0.025107397)]),\n",
       " (9,\n",
       "  [('extra-virgin olive oil', 0.113610104),\n",
       "   ('garlic cloves', 0.068074524),\n",
       "   ('fresh lemon juice', 0.059743356),\n",
       "   ('salt', 0.05615178),\n",
       "   ('olive oil', 0.053689852),\n",
       "   ('ground black pepper', 0.051845204),\n",
       "   ('plum tomatoes', 0.046660636),\n",
       "   ('purple onion', 0.042928956),\n",
       "   ('balsamic vinegar', 0.034868624),\n",
       "   ('fresh basil', 0.033855587)]),\n",
       " (10,\n",
       "  [('broccoli florets', 0.064656645),\n",
       "   ('button mushrooms', 0.049178652),\n",
       "   ('crème fraîche', 0.0480687),\n",
       "   ('yellow squash', 0.045775656),\n",
       "   ('radishes', 0.04377321),\n",
       "   ('greek style plain yogurt', 0.038285606),\n",
       "   ('pork sausages', 0.03810341),\n",
       "   ('watercress', 0.034755643),\n",
       "   ('quickcooking grits', 0.03248683),\n",
       "   ('ripe olives', 0.030701196)]),\n",
       " (11,\n",
       "  [('lime', 0.13813326),\n",
       "   ('lime juice', 0.082676515),\n",
       "   ('fresh cilantro', 0.057013955),\n",
       "   ('chopped cilantro', 0.039347593),\n",
       "   ('purple onion', 0.038651157),\n",
       "   ('mango', 0.0308998),\n",
       "   ('garlic', 0.030336991),\n",
       "   ('lime wedges', 0.028639974),\n",
       "   ('jalapeno chilies', 0.027963106),\n",
       "   ('salt', 0.023252666)]),\n",
       " (12,\n",
       "  [('cheese', 0.09667458),\n",
       "   ('ricotta cheese', 0.09322544),\n",
       "   ('orange juice', 0.08797204),\n",
       "   ('sliced mushrooms', 0.05943291),\n",
       "   ('baby spinach', 0.05845924),\n",
       "   ('vegetable broth', 0.05303407),\n",
       "   ('vegetable oil cooking spray', 0.052082267),\n",
       "   ('frozen chopped spinach', 0.037364386),\n",
       "   ('part-skim mozzarella cheese', 0.034633752),\n",
       "   ('italian sausage', 0.033636384)]),\n",
       " (13,\n",
       "  [('diced tomatoes', 0.099317186),\n",
       "   ('dried oregano', 0.06178454),\n",
       "   ('onions', 0.06136845),\n",
       "   ('tomato sauce', 0.051887263),\n",
       "   ('garlic', 0.047277674),\n",
       "   ('salt', 0.04642682),\n",
       "   ('tomato paste', 0.045648966),\n",
       "   ('olive oil', 0.03946867),\n",
       "   ('crushed tomatoes', 0.032827698),\n",
       "   ('ground beef', 0.02914092)]),\n",
       " (14,\n",
       "  [('tomatoes', 0.11689535),\n",
       "   ('salt', 0.09071603),\n",
       "   ('red wine vinegar', 0.083499454),\n",
       "   ('olive oil', 0.07141564),\n",
       "   ('cucumber', 0.06505049),\n",
       "   ('pepper', 0.055356186),\n",
       "   ('lemon juice', 0.0521993),\n",
       "   ('fresh oregano', 0.04325499),\n",
       "   ('garlic', 0.035337772),\n",
       "   ('purple onion', 0.033643)]),\n",
       " (15,\n",
       "  [('lemon', 0.20150474),\n",
       "   ('orange', 0.054860555),\n",
       "   ('boiling water', 0.052821387),\n",
       "   ('fine sea salt', 0.049109984),\n",
       "   ('sugar', 0.044908132),\n",
       "   ('cold water', 0.043299384),\n",
       "   ('fennel seeds', 0.039471664),\n",
       "   ('almonds', 0.03155838),\n",
       "   ('water', 0.03096701),\n",
       "   ('mint', 0.02787388)]),\n",
       " (16,\n",
       "  [('chopped cilantro fresh', 0.10233976),\n",
       "   ('fresh lime juice', 0.07429672),\n",
       "   ('jalapeno chilies', 0.068773985),\n",
       "   ('white onion', 0.051350173),\n",
       "   ('salt', 0.045733918),\n",
       "   ('avocado', 0.042336613),\n",
       "   ('garlic cloves', 0.037823476),\n",
       "   ('ground cumin', 0.03717741),\n",
       "   ('vegetable oil', 0.027913326),\n",
       "   ('cilantro leaves', 0.02776745)]),\n",
       " (17,\n",
       "  [('ground ginger', 0.10094584),\n",
       "   ('ground cinnamon', 0.0997863),\n",
       "   ('raisins', 0.08208242),\n",
       "   ('ground cloves', 0.077464946),\n",
       "   ('white wine', 0.07109761),\n",
       "   ('ground allspice', 0.061465643),\n",
       "   ('fresh mushrooms', 0.05536021),\n",
       "   ('lean ground beef', 0.05042736),\n",
       "   ('dried rosemary', 0.03250882),\n",
       "   ('iceberg lettuce', 0.030165061)]),\n",
       " (18,\n",
       "  [('parmesan cheese', 0.117866606),\n",
       "   ('warm water', 0.07295963),\n",
       "   ('salt', 0.063869424),\n",
       "   ('olive oil', 0.053459086),\n",
       "   ('dried basil', 0.04822012),\n",
       "   ('grits', 0.039346643),\n",
       "   ('kale', 0.032733694),\n",
       "   ('plain flour', 0.031273805),\n",
       "   ('water', 0.029299151),\n",
       "   ('dry yeast', 0.027152404)]),\n",
       " (19,\n",
       "  [('unsalted butter', 0.12123799),\n",
       "   ('all-purpose flour', 0.09647854),\n",
       "   ('large eggs', 0.0938753),\n",
       "   ('salt', 0.07580388),\n",
       "   ('sugar', 0.06174472),\n",
       "   ('whole milk', 0.039351795),\n",
       "   ('granulated sugar', 0.037839945),\n",
       "   ('baking powder', 0.033588972),\n",
       "   ('large egg yolks', 0.026013989),\n",
       "   ('buttermilk', 0.025799284)]),\n",
       " (20,\n",
       "  [('flat leaf parsley', 0.118042275),\n",
       "   ('freshly ground pepper', 0.10121337),\n",
       "   ('extra-virgin olive oil', 0.056730863),\n",
       "   ('garlic cloves', 0.052960094),\n",
       "   ('large shrimp', 0.0442805),\n",
       "   ('salt', 0.042346008),\n",
       "   ('olive oil', 0.042167712),\n",
       "   ('dry bread crumbs', 0.032805283),\n",
       "   ('ground black pepper', 0.026847152),\n",
       "   ('kosher salt', 0.024603887)]),\n",
       " (21,\n",
       "  [('chicken broth', 0.14803913),\n",
       "   ('green bell pepper', 0.06681109),\n",
       "   ('boneless skinless chicken breast halves', 0.06019626),\n",
       "   ('boneless skinless chicken breasts', 0.056340966),\n",
       "   ('onions', 0.056282617),\n",
       "   ('chicken breasts', 0.05011488),\n",
       "   ('red bell pepper', 0.042771254),\n",
       "   ('butter', 0.040118042),\n",
       "   ('pepper', 0.040042616),\n",
       "   ('salt', 0.035015985)]),\n",
       " (22,\n",
       "  [('grated parmesan cheese', 0.11367875),\n",
       "   ('zucchini', 0.06631977),\n",
       "   ('olive oil', 0.06301537),\n",
       "   ('salt', 0.04401548),\n",
       "   ('mozzarella cheese', 0.04062546),\n",
       "   ('garlic', 0.040482976),\n",
       "   ('shredded mozzarella cheese', 0.037527442),\n",
       "   ('eggplant', 0.036429554),\n",
       "   ('pepper', 0.033479728),\n",
       "   ('eggs', 0.032306578)]),\n",
       " (23,\n",
       "  [('brown sugar', 0.078391366),\n",
       "   ('water', 0.050638635),\n",
       "   ('salt', 0.05003976),\n",
       "   ('soy sauce', 0.0477935),\n",
       "   ('white pepper', 0.04608279),\n",
       "   ('oil', 0.04398763),\n",
       "   ('sugar', 0.040352862),\n",
       "   ('sauce', 0.038324192),\n",
       "   ('garlic', 0.034193605),\n",
       "   ('ketchup', 0.033582624)]),\n",
       " (24,\n",
       "  [('ground cumin', 0.074109286),\n",
       "   ('salt', 0.047322456),\n",
       "   ('ground coriander', 0.046991196),\n",
       "   ('curry powder', 0.039033882),\n",
       "   ('onions', 0.032947443),\n",
       "   ('garlic', 0.028365308),\n",
       "   ('vegetable oil', 0.027905172),\n",
       "   ('ground turmeric', 0.027181257),\n",
       "   ('garlic cloves', 0.026404317),\n",
       "   ('fresh ginger', 0.024133716)]),\n",
       " (25,\n",
       "  [('hot water', 0.08969456),\n",
       "   ('chopped garlic', 0.08335439),\n",
       "   ('peanut oil', 0.06530338),\n",
       "   ('rice wine', 0.05884086),\n",
       "   ('hot red pepper flakes', 0.045962963),\n",
       "   ('corn oil', 0.036611248),\n",
       "   ('fontina cheese', 0.032669272),\n",
       "   ('marsala wine', 0.03058514),\n",
       "   ('seasoning', 0.030315252),\n",
       "   ('garlic chili sauce', 0.029549731)]),\n",
       " (26,\n",
       "  [('mirin', 0.08190804),\n",
       "   ('chickpeas', 0.07192887),\n",
       "   ('red pepper', 0.050112758),\n",
       "   ('mint leaves', 0.04967677),\n",
       "   ('juice', 0.04816603),\n",
       "   ('chopped fresh mint', 0.047310207),\n",
       "   ('sugar', 0.03911479),\n",
       "   ('fresh coriander', 0.03617355),\n",
       "   ('sake', 0.035230346),\n",
       "   ('grated lemon zest', 0.029977176)]),\n",
       " (27,\n",
       "  [('heavy cream', 0.18389088),\n",
       "   ('cheddar cheese', 0.101529546),\n",
       "   ('frozen peas', 0.06820053),\n",
       "   ('grated nutmeg', 0.06580064),\n",
       "   ('bananas', 0.04166958),\n",
       "   ('bread', 0.036532246),\n",
       "   ('ice', 0.025883922),\n",
       "   ('adobo sauce', 0.023814833),\n",
       "   ('old bay seasoning', 0.02300393),\n",
       "   ('butter', 0.02249873)]),\n",
       " (28,\n",
       "  [('oil', 0.11665615),\n",
       "   ('salt', 0.0869403),\n",
       "   ('cilantro leaves', 0.053353395),\n",
       "   ('green chilies', 0.05182968),\n",
       "   ('cumin seed', 0.04763099),\n",
       "   ('onions', 0.04659431),\n",
       "   ('ground turmeric', 0.042325657),\n",
       "   ('water', 0.040512484),\n",
       "   ('chili powder', 0.033063885),\n",
       "   ('tomatoes', 0.029588612)]),\n",
       " (29,\n",
       "  [('sour cream', 0.07688275),\n",
       "   ('salsa', 0.046891417),\n",
       "   ('flour tortillas', 0.046290774),\n",
       "   ('chili powder', 0.045407046),\n",
       "   ('shredded cheddar cheese', 0.04090793),\n",
       "   ('corn tortillas', 0.04089428),\n",
       "   ('black beans', 0.039927486),\n",
       "   ('cilantro', 0.035491474),\n",
       "   ('salt', 0.029888315),\n",
       "   ('ground cumin', 0.028547049)]),\n",
       " (30,\n",
       "  [('sugar', 0.092246786),\n",
       "   ('whipping cream', 0.0801006),\n",
       "   ('egg yolks', 0.07596768),\n",
       "   ('egg whites', 0.055294696),\n",
       "   ('vanilla extract', 0.05012538),\n",
       "   ('butter', 0.050104946),\n",
       "   ('half & half', 0.042696152),\n",
       "   ('sweetened condensed milk', 0.036738083),\n",
       "   ('water', 0.033754878),\n",
       "   ('strawberries', 0.030936928)]),\n",
       " (31,\n",
       "  [('shrimp', 0.09856105),\n",
       "   ('medium shrimp', 0.060185313),\n",
       "   ('vegetable oil', 0.043311708),\n",
       "   ('long-grain rice', 0.043023568),\n",
       "   ('green onions', 0.040445913),\n",
       "   ('long grain white rice', 0.03362561),\n",
       "   ('rice noodles', 0.033340517),\n",
       "   ('cajun seasoning', 0.03258463),\n",
       "   ('hot pepper sauce', 0.029391613),\n",
       "   ('scallions', 0.028787484)]),\n",
       " (32,\n",
       "  [('eggs', 0.11195183),\n",
       "   ('milk', 0.104218975),\n",
       "   ('salt', 0.096027546),\n",
       "   ('butter', 0.072243154),\n",
       "   ('all-purpose flour', 0.06717707),\n",
       "   ('flour', 0.04234899),\n",
       "   ('baking powder', 0.04103165),\n",
       "   ('white sugar', 0.04056095),\n",
       "   ('sugar', 0.040244028),\n",
       "   ('water', 0.023008436)]),\n",
       " (33,\n",
       "  [('rice', 0.060697865),\n",
       "   ('coriander', 0.053293504),\n",
       "   ('onions', 0.04881674),\n",
       "   ('salt', 0.046873853),\n",
       "   ('garam masala', 0.042082515),\n",
       "   ('tumeric', 0.040866077),\n",
       "   ('ginger', 0.03973486),\n",
       "   ('cabbage', 0.03836692),\n",
       "   ('garlic', 0.035163242),\n",
       "   ('ghee', 0.03287601)]),\n",
       " (34,\n",
       "  [('cinnamon sticks', 0.091731824),\n",
       "   ('clove', 0.08541796),\n",
       "   ('black peppercorns', 0.071522504),\n",
       "   ('chopped tomatoes', 0.04195028),\n",
       "   ('cream', 0.04176324),\n",
       "   ('garlic paste', 0.03891573),\n",
       "   ('coriander seeds', 0.035171386),\n",
       "   ('yoghurt', 0.035050035),\n",
       "   ('fresh dill', 0.032886796),\n",
       "   ('onions', 0.028288862)]),\n",
       " (35,\n",
       "  [('onions', 0.06829174),\n",
       "   ('bay leaves', 0.05227054),\n",
       "   ('carrots', 0.052168127),\n",
       "   ('celery', 0.04998025),\n",
       "   ('salt', 0.049618628),\n",
       "   ('bay leaf', 0.045024183),\n",
       "   ('dried thyme', 0.043703984),\n",
       "   ('water', 0.042502213),\n",
       "   ('garlic', 0.033353377),\n",
       "   ('ground black pepper', 0.02980959)]),\n",
       " (36,\n",
       "  [('salt', 0.112778164),\n",
       "   ('pepper', 0.08981265),\n",
       "   ('paprika', 0.07362888),\n",
       "   ('onions', 0.06963493),\n",
       "   ('potatoes', 0.056682687),\n",
       "   ('butter', 0.04309529),\n",
       "   ('garlic', 0.038376823),\n",
       "   ('olive oil', 0.034838535),\n",
       "   ('worcestershire sauce', 0.028210223),\n",
       "   ('parsley', 0.022734864)]),\n",
       " (37,\n",
       "  [('sea salt', 0.10950181),\n",
       "   ('coarse salt', 0.06573789),\n",
       "   ('crushed red pepper flakes', 0.05671413),\n",
       "   ('extra-virgin olive oil', 0.051381934),\n",
       "   ('ground black pepper', 0.045456987),\n",
       "   ('celery ribs', 0.043223705),\n",
       "   ('ground pepper', 0.041298624),\n",
       "   ('kosher salt', 0.039006002),\n",
       "   ('parmigiano reggiano cheese', 0.03378269),\n",
       "   ('garlic cloves', 0.032566294)]),\n",
       " (38,\n",
       "  [('fish sauce', 0.115157686),\n",
       "   ('coconut milk', 0.055713482),\n",
       "   ('garlic', 0.04196318),\n",
       "   ('red chili peppers', 0.037942067),\n",
       "   ('shallots', 0.037201572),\n",
       "   ('lemongrass', 0.03437952),\n",
       "   ('sugar', 0.028327404),\n",
       "   ('vegetable oil', 0.025610782),\n",
       "   ('boneless chicken skinless thigh', 0.025040459),\n",
       "   ('water', 0.024432255)]),\n",
       " (39,\n",
       "  [('mayonaise', 0.1526512),\n",
       "   ('dijon mustard', 0.09494172),\n",
       "   ('cider vinegar', 0.06907957),\n",
       "   ('cracked black pepper', 0.06900843),\n",
       "   ('roma tomatoes', 0.046642147),\n",
       "   ('white rice', 0.043439806),\n",
       "   ('lemon wedge', 0.04310416),\n",
       "   ('romaine lettuce', 0.036114544),\n",
       "   ('green onions', 0.024727147),\n",
       "   ('chicken wings', 0.024276901)])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_lda_topics_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salt': 24, 'sugar': 9, 'water': 9, 'mushrooms': 1, 'chicken': 0, 'eggs': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answer(words):\n",
    "    answer = {i:0 for i in words}\n",
    "    for word in words:\n",
    "        for topic in temp_lda_topics_0:\n",
    "            for tup in topic[1]:\n",
    "                if word == tup[0]:\n",
    "                    answer[word] += 1\n",
    "    return answer\n",
    "\n",
    "\n",
    "words = [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]\n",
    "get_answer(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))\n",
    "\n",
    "save_answers1(5, 2, 4, 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 15, 11, 18, 20, 29, 44, 52, 59, 104, 114]\n",
      "6714\n",
      "6702\n"
     ]
    }
   ],
   "source": [
    "d = list(dict(filter(lambda elem: elem[1] > 4000, dictionary2.dfs.items())))\n",
    "print(d)\n",
    "print(len(dictionary2.dfs))\n",
    "dictionary2.filter_tokens(bad_ids=d)\n",
    "print(len(dictionary2.dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428249\n",
      "343665\n"
     ]
    }
   ],
   "source": [
    "def calculate_corp_size(corp):\n",
    "    count = 0\n",
    "    for elem in corp:\n",
    "        count += len(elem)\n",
    "    return count\n",
    "print(calculate_corp_size(corpus))\n",
    "print(calculate_corp_size(corpus2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))\n",
    "\n",
    "save_answers2(6714, 6702, 428249, 343665)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "lda2 = models.LdaModel(corpus2, num_topics=40, passes=5, id2word=dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh2 = lda2.top_topics(corpus2)\n",
    "coh = lda.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.364392784796212\n",
      "-8.596629015979598\n"
     ]
    }
   ],
   "source": [
    "def calculate_coherence(coherence):\n",
    "    return sum([elem[1] for elem in coherence]) / len(coherence)\n",
    "print(calculate_coherence(coh))\n",
    "print(calculate_coherence(coh2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))\n",
    "\n",
    "save_answers3(-6.364392784796212, -8.596629015979598)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0.12812185), (31, 0.6175929), (33, 0.13865705)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.get_document_topics(corpus2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda3 = models.LdaModel(corpus2, num_topics=40, passes=5, id2word=dictionary2, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203661"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda3.get_document_topics(corpus2, minimum_probability=0.01)[0]\n",
    "def get_sum(doc_topic):\n",
    "    return sum([len(i) for i in doc_topic])\n",
    "\n",
    "get_sum(lda2.get_document_topics(corpus2, minimum_probability=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda3.get_document_topics(corpus2, minimum_probability=0.01)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetta = lda2.get_document_topics(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(114, 1),\n",
       " (123, 1),\n",
       " (267, 1),\n",
       " (319, 1),\n",
       " (465, 1),\n",
       " (466, 1),\n",
       " (467, 1),\n",
       " (468, 1),\n",
       " (469, 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = [elem['cuisine'] for elem in recipes]\n",
    "corpus2[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-77a50a800c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m76543\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtetta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtetta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             )\n\u001b[0;32m--> 303\u001b[0;31m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[1;32m    304\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    797\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(tetta, y)\n",
    "print(cross_val_score(clf, tetta, y, cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
